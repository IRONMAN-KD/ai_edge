#!/usr/bin/env python3
"""
AI Edgeç»Ÿä¸€ç‰ˆæœ¬æœ¬åœ°è¿è¡Œè„šæœ¬
ç”¨äºå¿«é€Ÿå¯åŠ¨å’Œæµ‹è¯•ç³»ç»Ÿ
"""

import os
import sys
import logging
import argparse

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    try:
        import fastapi
        import uvicorn
        import numpy
        import cv2
        import onnxruntime
        print("âœ… åŸºç¡€ä¾èµ–æ£€æŸ¥é€šè¿‡")
        return True
    except ImportError as e:
        print(f"âŒ ä¾èµ–æ£€æŸ¥å¤±è´¥: {e}")
        print("è¯·è¿è¡Œ: pip install -r requirements/base.txt -r requirements/cpu.txt")
        return False

def check_model():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶"""
    print("ğŸ” æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
    
    model_path = "models/onnx/person.onnx"
    if os.path.exists(model_path):
        print(f"âœ… æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        return model_path
    else:
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None

def test_inference_engine():
    """æµ‹è¯•æ¨ç†å¼•æ“"""
    print("ğŸ” æµ‹è¯•æ¨ç†å¼•æ“...")
    
    try:
        from inference.factory import InferenceFactory
        from utils.platform_detector import auto_detect_platform
        
        # è‡ªåŠ¨æ£€æµ‹å¹³å°
        platform = auto_detect_platform()
        print(f"âœ… æ£€æµ‹åˆ°å¹³å°: {platform}")
        
        # æ£€æŸ¥æ”¯æŒçš„å¹³å°
        supported = InferenceFactory.get_supported_platforms()
        print(f"âœ… æ”¯æŒçš„å¹³å°: {supported}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¼•æ“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AI Edgeæœ¬åœ°è¿è¡Œè„šæœ¬")
    parser.add_argument("--port", type=int, default=8000, help="APIæœåŠ¡ç«¯å£")
    parser.add_argument("--host", default="0.0.0.0", help="APIæœåŠ¡ä¸»æœº")
    parser.add_argument("--platform", help="æŒ‡å®šå¹³å° (cpu_x86, cpu_arm, etc.)")
    parser.add_argument("--check-only", action="store_true", help="ä»…æ£€æŸ¥ç¯å¢ƒï¼Œä¸å¯åŠ¨æœåŠ¡")
    
    args = parser.parse_args()
    
    setup_logging()
    
    print("ğŸš€ AI Edgeç»Ÿä¸€ç‰ˆæœ¬æœ¬åœ°è¿è¡Œ")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return 1
    
    # æ£€æŸ¥æ¨¡å‹
    model_path = check_model()
    if not model_path:
        return 1
    
    # æµ‹è¯•æ¨ç†å¼•æ“
    if not test_inference_engine():
        return 1
    
    if args.check_only:
        print("âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆï¼Œæ‰€æœ‰ç»„ä»¶æ­£å¸¸")
        return 0
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    if args.platform:
        os.environ['PLATFORM'] = args.platform
    
    # å¯åŠ¨APIæœåŠ¡
    print(f"ğŸš€ å¯åŠ¨APIæœåŠ¡ (http://{args.host}:{args.port})")
    
    try:
        import uvicorn
        uvicorn.run(
            "api.main:app",
            host=args.host,
            port=args.port,
            reload=True,
            log_level="info"
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")
    except Exception as e:
        print(f"âŒ æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    sys.exit(main()) 